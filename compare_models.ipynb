{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Klassifikation des Objekttyps (1/4)\n",
    "Ziel dieser Aufgabe ist es, ein Modell zu erstellen, welches durch beliebige Metriken eine akkurate Vorhersage über den Typ der Immobilie erstellt.\n",
    "\n",
    "Wir vergleichen hier drei Ansätze miteinander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der benötigten Libraries und Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "30a8a31271d943e3810720b68a433c99",
    "deepnote_cell_height": 345,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2123,
    "execution_start": 1666355025247,
    "source_hash": "54b48820",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.gabonisator import Gabonisator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten beziehen wir aus unserem Data Repository auf GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-ed06f97a-4428-4307-a24a-409601fb792a",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 298,
    "execution_start": 1666355027374,
    "source_hash": "8f6ec427"
   },
   "outputs": [],
   "source": [
    "# read clean data + plz data\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/Immobilienrechner-Challenge/data/main/clean.csv\"\n",
    ")\n",
    "data = data[[\"price\", \"zip_code\", \"living_space\", \"rooms\", \"type\"]]\n",
    "plz = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/Immobilienrechner-Challenge/data/main/plz_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unsere kategorialen Zielvariable Type vorherzusagen, müssen wir sie in Dummies umwandeln, da wir nur numerische Werte berechnen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst kombinieren wir unseren reduzierten Wohnungsdatensatz mit unserem Postleitzahl Datensatz, um weitere Features zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b24ebf59e0504eacb6d4d8f1ce09fcec",
    "deepnote_cell_height": 731,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     20
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 151,
    "execution_start": 1666355027685,
    "source_hash": "1912f9aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join clean data with plz data on plz\n",
    "data = pd.get_dummies(data, columns=[\"type\"])\n",
    "data = data.merge(plz, how=\"inner\", left_on=\"zip_code\", right_on=\"PLZ\")\n",
    "data = data.drop(columns=[\"PLZ\", \"zip_code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da unser Datensatz nur wenig NAs hat, verwerfen wir diese Observationen. Uns bleiben noch um die 11000 Datensätze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3452170aaf3a4c8ea5d5cdbefe0895c4",
    "deepnote_cell_height": 130,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     20
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1666355027883,
    "source_hash": "7e7d9889",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop NAs\n",
    "data = data.dropna()\n",
    "# print new number of observations\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Daten spalten wir danach in Features und Zielvariablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-956099e5-782e-4e8a-865b-e923f2147947",
    "deepnote_cell_height": 148,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1666355027884,
    "source_hash": "cb6f131b"
   },
   "outputs": [],
   "source": [
    "# define columns for X and y\n",
    "y_cols = data.columns[data.columns.str.startswith(\"type_\")]\n",
    "X_cols = list(set(data._get_numeric_data().columns) - set(y_cols))\n",
    "\n",
    "# convert pandas data to numpy arrays\n",
    "X = data[X_cols].values\n",
    "y = data[y_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach skalieren wir unsere Features mit einem StandardScaler auf die Werte aus unserem ganzen Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-323c4c4d-d773-497f-9854-c40bb318c7c9",
    "deepnote_cell_height": 94,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 43,
    "execution_start": 1666355027884,
    "source_hash": "9010a866"
   },
   "outputs": [],
   "source": [
    "# scale data to whole dataset\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Daten unterteilen wir danach auf Trainings- und Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and val data (70%/30%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wandeln wir unsere Numpy Werte in Float32 Werte, damit die Berechnung durch PyTorch effizienter verlauft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors from numpy arrays\n",
    "X_train_pytorch = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_val_pytorch = torch.from_numpy(X_val.astype(np.float32))\n",
    "y_train_pytorch = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_val_pytorch = torch.from_numpy(y_val.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Architektur unseres neuronalen Netzes sieht so aus:\n",
    "\n",
    "<svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move; background: white;\" width=\"380\" height=\"185\"><g transform=\"translate(-743.5554710569938,-326.8616196843533)scale(1.1)\"><polygon class=\"poly\" id=\"fc_0\" style=\"fill: rgb(224, 224, 224); stroke: black; stroke-width: 0.67px; opacity: 0.45;\" points=\"718.5,344.6489719263484 728.5,344.6489719263484 828.2020561473032,444.3510280736516 818.2020561473032,444.3510280736516\"></polygon><polygon class=\"poly\" id=\"fc_1\" style=\"fill: rgb(224, 224, 224); stroke: black; stroke-width: 0.67px; opacity: 0.45;\" points=\"830.2020561473032,371.87258300203047 840.2020561473032,371.87258300203047 885.4568901432423,417.1274169979695 875.4568901432423,417.1274169979695\"></polygon><polygon class=\"poly\" id=\"fc_2\" style=\"fill: rgb(224, 224, 224); stroke: black; stroke-width: 0.67px; opacity: 0.45;\" points=\"887.4568901432422,371.87258300203047 897.4568901432422,371.87258300203047 942.7117241391812,417.1274169979695 932.7117241391812,417.1274169979695\"></polygon><polygon class=\"poly\" id=\"fc_3\" style=\"fill: rgb(224, 224, 224); stroke: black; stroke-width: 0.67px; opacity: 0.45;\" points=\"944.7117241391812,387.07537879754125 954.7117241391812,387.07537879754125 969.5609665440987,401.92462120245875 959.5609665440987,401.92462120245875\"></polygon><line class=\"line\" id=\"fc_0\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 0;\" x1=\"NaN\" y1=\"NaN\" x2=\"818.2020561473032\" y2=\"444.3510280736516\"></line><line class=\"line\" id=\"fc_0\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 0;\" x1=\"NaN\" y1=\"NaN\" x2=\"718.5\" y2=\"344.6489719263484\"></line><line class=\"line\" id=\"fc_1\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 1;\" x1=\"828.2020561473032\" y1=\"444.3510280736516\" x2=\"875.4568901432422\" y2=\"417.1274169979695\"></line><line class=\"line\" id=\"fc_1\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 1;\" x1=\"728.5\" y1=\"344.6489719263484\" x2=\"830.2020561473032\" y2=\"371.87258300203047\"></line><line class=\"line\" id=\"fc_2\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 1;\" x1=\"885.4568901432422\" y1=\"417.1274169979695\" x2=\"932.7117241391812\" y2=\"417.1274169979695\"></line><line class=\"line\" id=\"fc_2\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 1;\" x1=\"840.2020561473032\" y1=\"371.87258300203047\" x2=\"887.4568901432422\" y2=\"371.87258300203047\"></line><line class=\"line\" id=\"fc_3\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 1;\" x1=\"942.7117241391812\" y1=\"417.1274169979695\" x2=\"959.5609665440987\" y2=\"401.92462120245875\"></line><line class=\"line\" id=\"fc_3\" style=\"stroke: black; stroke-width: 0.335px; stroke-opacity: 0.45; opacity: 1;\" x1=\"897.4568901432422\" y1=\"371.87258300203047\" x2=\"944.7117241391812\" y2=\"387.07537879754125\"></line><text class=\"info\" dy=\"-0.3em\" style=\"font-size: 16px;\" font-family=\"sans-serif\" x=\"718.5\" y=\"329.6489719263484\">1x141</text><text class=\"info\" dy=\"-0.3em\" style=\"font-size: 16px;\" font-family=\"sans-serif\" x=\"830.2020561473032\" y=\"356.87258300203047\">1x64</text><text class=\"info\" dy=\"-0.3em\" style=\"font-size: 16px;\" font-family=\"sans-serif\" x=\"887.4568901432422\" y=\"356.87258300203047\">1x64</text><text class=\"info\" dy=\"-0.3em\" style=\"font-size: 16px;\" font-family=\"sans-serif\" x=\"944.7117241391812\" y=\"372.07537879754125\">1x21</text></g></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Modell erstellen wir danach und definieren eine Learning Rate, eine Lossfunction und ein Optimizer.\n",
    "Als Lossfunction nehmen wir die Binary-Crossentropy und als Optimizer Adam.\n",
    "Danach trainieren wir unser Modell und speichern unsere Losses nach jeder Epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Gabonisator(X.shape[1], y.shape[1])\n",
    "\n",
    "# define learning rate, loss function and optimizer\n",
    "learning_rate = 0.00003\n",
    "lossf = torch.nn.BCELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss tracker\n",
    "loss_train_arr = []\n",
    "loss_val_arr = []\n",
    "\n",
    "# define iterations and iterate (training)\n",
    "n_epochs = 10000\n",
    "for epoch in range(n_epochs):\n",
    "    # forward prop\n",
    "    y_pred = model(X_train_pytorch)\n",
    "    # loss function\n",
    "    loss_train = lossf(y_pred, y_train_pytorch)\n",
    "    # back prop\n",
    "    loss_train.backward()\n",
    "    # update params with optimizer\n",
    "    optim.step()\n",
    "    # empty gradients\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # forward prop val data\n",
    "    y_pred_val = model(X_val_pytorch)\n",
    "    # get loss of validation data\n",
    "    loss_val = lossf(y_pred_val, y_val_pytorch)\n",
    "    # save losses\n",
    "    loss_train_arr.append(loss_train.item())\n",
    "    loss_val_arr.append(loss_val.item())\n",
    "\n",
    "    # print info every 500 epochs\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {(epoch + 1): >5}/{n_epochs} | Loss train: {loss_train_arr[-1]:.4f} | Loss val: {loss_val_arr[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "# print final data\n",
    "with torch.no_grad():\n",
    "    print(\"---\")\n",
    "    print(f\"Final Run\")\n",
    "    print(\n",
    "        f\"Epoch: {(epoch + 1): >5}/{n_epochs} | Loss train: {loss_train_arr[-1]:.4f} | Loss val: {loss_val_arr[-1]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   Best val loss: {np.min(loss_val_arr):.4f} at Epoch {np.argmin(loss_val_arr) + 1}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen Überblick zu erhalten, wie gut das Modell jetzt trainiert wurde, visualisieren wir unsere Losses nach jeder Epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize score\n",
    "ymin = np.min(loss_val_arr) - 0.05\n",
    "ymax = np.max(loss_val_arr) + 0.15\n",
    "plt.figure(figsize=(16, 6), dpi=200)\n",
    "plt.title(\"detecting overfitting via train and validation losses\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"losses\")\n",
    "plt.axvspan(\n",
    "    np.argmin(loss_val_arr),\n",
    "    len(loss_val_arr),\n",
    "    (np.min(loss_val_arr) - ymin) / (ymax - ymin),\n",
    "    1,\n",
    "    color=\"#F5F200\",\n",
    "    alpha=0.3,\n",
    "    label=\"overfitting\",\n",
    ")\n",
    "plt.axvspan(\n",
    "    0,\n",
    "    len(loss_val_arr),\n",
    "    0,\n",
    "    (np.min(loss_val_arr) - ymin) / (ymax - ymin),\n",
    "    color=\"#52F748\",\n",
    "    alpha=0.2,\n",
    "    label=\"optimizing potential\",\n",
    ")\n",
    "plt.plot(loss_val_arr, color=\"#9166FF\", label=\"loss validation\")\n",
    "plt.plot(loss_train_arr, color=\"#E02420\", label=\"loss train\")\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlim(0, len(loss_val_arr))\n",
    "plt.xticks([np.argmin(loss_val_arr)])\n",
    "plt.yticks([np.min(loss_val_arr)])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir hier erkennen können, passiert nach einer bestimmten Zahl Iterationen overfitting, da der Loss Wert unseres Validierungsdatensatzes steigt, anstatt zu sinken. Um dies zu bekämpfen, muss das Modell selbst erkennen, dass Overfitting stattfindet und frühzeitig abbrechen. Dafür implemntieren wir ein Early Stopping, welches überprüft, ob der letzte Loss Wert kleiner ist, als der aktuelle. Falls dies der Fall ist, hört das Modell auf zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Gabonisator(X.shape[1], y.shape[1])\n",
    "\n",
    "# define learning rate, loss function and optimizer\n",
    "learning_rate = 0.00003\n",
    "lossf = torch.nn.BCELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss tracker\n",
    "loss_train_arr = []\n",
    "loss_val_arr = []\n",
    "\n",
    "# define iterations and iterate (training)\n",
    "n_epochs = 10000\n",
    "for epoch in range(n_epochs):\n",
    "    # forward prop\n",
    "    y_pred = model(X_train_pytorch)\n",
    "    # loss function\n",
    "    loss_train = lossf(y_pred, y_train_pytorch)\n",
    "    # back prop\n",
    "    loss_train.backward()\n",
    "    # update params with optimizer\n",
    "    optim.step()\n",
    "    # empty gradients\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # forward prop val data\n",
    "    y_pred_val = model(X_val_pytorch)\n",
    "    # get loss of validation data\n",
    "    loss_val = lossf(y_pred_val, y_val_pytorch)\n",
    "    # save losses\n",
    "    loss_train_arr.append(loss_train.item())\n",
    "    loss_val_arr.append(loss_val.item())\n",
    "\n",
    "    # loss val goes up too much -> EarlyStop\n",
    "    if np.round(np.min(loss_val_arr), 4) != np.round(loss_val.item(), 4):\n",
    "        print(\"Early Stopping!\")\n",
    "        break\n",
    "\n",
    "    # print info every 500 epochs\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {(epoch + 1): >5}/{n_epochs} | Loss train: {loss_train_arr[-1]:.4f} | Loss val: {loss_val_arr[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "# print final data\n",
    "with torch.no_grad():\n",
    "    accuracy = (\n",
    "        (model(X_val_pytorch).argmax(axis=1) == y_val_pytorch.argmax(axis=1)).sum() / y_val.shape[0]\n",
    "    ).item()\n",
    "    print(\"---\")\n",
    "    print(f\"Final Run\")\n",
    "    print(\n",
    "        f\"Epoch: {(epoch): >5}/{n_epochs} | Loss train: {loss_train_arr[-1]:.4f} | Loss val: {loss_val_arr[-1]:.4f} | Score: {accuracy:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt visualisieren wir unsere Ergebnisse anhand der gleichen Visualisierung wie vorhin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize score\n",
    "ymin = np.min(loss_val_arr) - 0.05\n",
    "ymax = np.max(loss_val_arr) + 0.15\n",
    "plt.figure(figsize=(16, 6), dpi=200)\n",
    "plt.title(\"detecting overfitting via train and validation losses\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"losses\")\n",
    "plt.axvspan(\n",
    "    np.argmin(loss_val_arr),\n",
    "    len(loss_val_arr),\n",
    "    (np.min(loss_val_arr) - ymin) / (ymax - ymin),\n",
    "    1,\n",
    "    color=\"#F5F200\",\n",
    "    alpha=0.3,\n",
    "    label=\"overfitting\",\n",
    ")\n",
    "plt.axvspan(\n",
    "    0,\n",
    "    len(loss_val_arr),\n",
    "    0,\n",
    "    (np.min(loss_val_arr) - ymin) / (ymax - ymin),\n",
    "    color=\"#52F748\",\n",
    "    alpha=0.2,\n",
    "    label=\"optimizing potential\",\n",
    ")\n",
    "plt.plot(loss_val_arr, color=\"#9166FF\", label=\"loss validation\")\n",
    "plt.plot(loss_train_arr, color=\"#E02420\", label=\"loss train\")\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlim(0, len(loss_val_arr))\n",
    "plt.xticks([np.argmin(loss_val_arr)])\n",
    "plt.yticks([np.min(loss_val_arr)])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_for_continous_multioutput(y_pred_continous, y_true, score_type):\n",
    "    '''\n",
    "    Compute different scores where y_pred is a continuous-multioutput\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f1_score or precision_score or recall_scource from sklearn.metrics\n",
    "    '''\n",
    "    indices = [np.argmax(x) for x in y_pred_continous]\n",
    "    zeros = [np.zeros(len(x)) for x in y_pred_continous]\n",
    "    y_pred = list()\n",
    "    for x, ind in zip(zeros, indices):\n",
    "        x[ind] = 1\n",
    "        y_pred.append(x)\n",
    "    if score_type == \"f1_score\":\n",
    "        return f1_score(y_true, y_pred, average=\"weighted\", zero_division=1)\n",
    "    elif score_type == \"precision_score\":\n",
    "        return precision_score(y_true, y_pred, average=\"weighted\", zero_division=1)\n",
    "    elif score_type == \"recall_score\":\n",
    "        return recall_score(y_true, y_pred, average=\"weighted\", zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_np = model(X_val_pytorch).detach().numpy()\n",
    "y_val_np = y_val_pytorch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "    Unser {} bricht jetzt frühzeitig ab und verhindert somit overfitting.\\n\n",
    "    Unsere {} ist mit diesem Modell ca. {}%,\\n\n",
    "    Unser {} ist mit diesem Modell ca. {}%,\\n\n",
    "    Unsere {} ist mit diesem Modell ca. {}%,\\n\n",
    "    Unser {} ist mit diesem Modell ca. {}%\n",
    "    \"\"\".format(\n",
    "        model.__class__.__name__,\n",
    "        \"Accuracy\",\n",
    "        np.round(accuracy, 2),\n",
    "        \"F1 Score\",\n",
    "        np.round(get_score_for_continous_multioutput(y_pred_np, y_val_np, \"f1_score\"), 2),\n",
    "        \"Precision\",\n",
    "        np.round(get_score_for_continous_multioutput(y_pred_np, y_val_np, \"precision_score\"), 2),\n",
    "        \"Recall\",\n",
    "        np.round(get_score_for_continous_multioutput(y_pred_np, y_val_np, \"recall_score\"), 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.read_csv(\"https://raw.githubusercontent.com/Immobilienrechner-Challenge/data/main/clean.csv\")\n",
    "data_ = data_[['price', 'zip_code', 'living_space', 'rooms', 'type']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da unser Datensatz nur wenig NAs hat, verwerfen wir diese Observationen. Uns bleiben noch um die 11000 Datensätze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data_.dropna()\n",
    "len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Daten spalten wir danach in Features und Zielvariablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = [\"type\"]\n",
    "X_cols = list(set(data_.columns) - set(y_cols))\n",
    "\n",
    "X = data_[X_cols].values\n",
    "y = data_[y_cols].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Daten unterteilen wir danach auf Trainings- und Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier trainieren wir mehrere Modelle, welche wir anhand verschiedener Anzahl Neighbors erstellen. Für jedes Modell berechnen wir die Accuracy und speichern diese ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.array(\n",
    "    [\n",
    "        (i, KNeighborsClassifier(i).fit(X_train, y_train).score(X_val, y_val)) for i in tqdm(range(1, 50))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu entscheiden, wieviel Neighbors man für das Modell nehmen soll, visualisieren wir hier die Accuracy für jeden Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6), dpi=200)\n",
    "plt.title(\"Determining which number of neighbors results in the best accuracy\")\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy with validation data\")\n",
    "plt.plot(accuracies[:,0], accuracies[:,1])\n",
    "plt.xticks(np.linspace(1, 49, 9))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = int(accuracies[:, :1].ravel()[np.argmax(accuracies[:, 1:])])\n",
    "clf = OneVsRestClassifier(KNeighborsClassifier(neighbors)).fit(X_train, y_train)\n",
    "accuracy = clf.score(X_val, y_val)\n",
    "f1score = f1_score(clf.predict(X_val), y_val, average=\"weighted\", zero_division=1)\n",
    "precision = precision_score(clf.predict(X_val), y_val, average=\"weighted\", zero_division=1)\n",
    "recall = recall_score(clf.predict(X_val), y_val, average=\"weighted\", zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "    Aus dem obigen Plot kann man sehen, welches Modell die höchste Accuracy hat.\\n\n",
    "    Wir setzen für den {} die Anzhal {} = {}\\n\n",
    "    und erhalten damit eine {} von ca. {}%,\\n\n",
    "    Unser {} ist mit diesem Modell ca. {}%,\\n\n",
    "    Unsere {} ist mit diesem Modell ca. {}%,\\n\n",
    "    Unser {} ist mit diesem Modell ca. {}%\n",
    "    '''.format(\n",
    "        model.__class__.__name__,\n",
    "        \"neighbors\",\n",
    "        neighbors,\n",
    "        \"Accuracy\",\n",
    "        np.round(accuracy, 2),\n",
    "        \"F1 Score\",\n",
    "        np.round(f1score, 2),\n",
    "        \"Precision\",\n",
    "        np.round(precision, 2),\n",
    "        \"Recall\",\n",
    "        np.round(recall, 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu entscheiden welche Parameter in Frage kommen, probieren wir es zuerst mit \"RandomizedSearchCV\" und danach mit \"GridSearchCV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Hyperparameter Grid:<br />\n",
    "Wir erstellen zuerst einen Hyperparameter Grid,<br />\n",
    "und wählen nach dem Zufallsprinzip Kombinationen, um eine breite Palette von Werten auszuprobieren.<br />\n",
    "Danach instanzieren wir random search auf unsere Daten und wählen n_iter für die Nummer der verschiedenen<br />\n",
    "Kombinationen sowie cv für die Nummer der folds für Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Nummer der Bäume im Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 50, num = 10)]\n",
    "# Anzahl der Features, die bei jedem Split berücksichtigt werden\n",
    "max_features = ['sqrt']\n",
    "# Maximale Anzahl an Ebenen im Baum\n",
    "max_depth = [int(x) for x in np.linspace(1, 11, num = 1)]\n",
    "max_depth.append(None)\n",
    "# Mindestanzahl an Samples, die benötigt werden, um einen Knoten zu splitten\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Mindestanzahl an Samples, die benötigt werden, um einen Blattknoten zu erstellen\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Methode, um Samples für das Training jedes Baumes auszuwählen\n",
    "bootstrap = [True, False]\n",
    "# Erstelle das zufällige Grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "# Verwende das zufällige Grid, um die besten Hyperparameter zu finden\n",
    "# Erstelle zuerst das Basismodell, das angepasst werden soll\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "# Zufällige Suche nach Parametern, mit 3-facher Kreuzvalidierung,\n",
    "# Suche über 100 verschiedene Kombinationen und verwende alle verfügbaren Kerne\n",
    "rfc_random = RandomizedSearchCV(\n",
    "    estimator = random_forest_classifier,\n",
    "    param_distributions = random_grid,\n",
    "    n_iter = 50, \n",
    "    cv = 2, \n",
    "    verbose=0, \n",
    "    random_state=42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "# Passe das zufällige Suchmodell an\n",
    "rfc_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search mit Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Erstelle das Parametergrid basierend auf den Ergebnissen der zufälligen Suche\n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [1, None],\n",
    "    'max_features': [\"sqrt\"],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [10, 14, 18, 23, 27, 32, 36, 41, 45, 50]\n",
    "}\n",
    "# Erstelle ein Basismodell\n",
    "rf = RandomForestClassifier()\n",
    "# Erstelle das Grid-Suchmodell\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = rf, \n",
    "    param_grid = param_grid, \n",
    "    cv = 2, \n",
    "    n_jobs = -1, \n",
    "    verbose = 0\n",
    ")\n",
    "# Passe die Grid-Suche an die Daten an\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechne die höchste Accuracy für beide Hyperparameter Optimierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_random_accuracy = rfc_random.score(X_val, y_val)\n",
    "grid_accuracy = grid_search.best_estimator_.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printe alle erforderlichen Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = max(grid_search.best_params_[\"n_estimators\"], rfc_random.best_params_[\"n_estimators\"])\n",
    "model = grid_search.best_estimator_\n",
    "accuracy = max(rfc_random_accuracy, grid_accuracy)\n",
    "f1score = f1_score(model.predict(X_val), y_val, average=\"weighted\", zero_division=1)\n",
    "precision = precision_score(model.predict(X_val), y_val, average=\"weighted\", zero_division=1)\n",
    "recall = recall_score(model.predict(X_val), y_val, average=\"weighted\", zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "    Aus den obigen Überlegungen setzen wir\\n \n",
    "    für den {} den Parameter {} = {} (number of trees in the model)\\n\n",
    "    und erhalten damit eine {} von ca. {}%,\\n\n",
    "    unser {} ist mit diesem Modell ca. {}%,\\n\n",
    "    unsere {} ist mit diesem Modell ca. {}%,\\n\n",
    "    unser {} ist mit diesem Modell ca. {}%\n",
    "    '''.format(\n",
    "        model.__class__.__name__,\n",
    "        \"n_estimators\",\n",
    "        n_estimators,\n",
    "        \"Accuracy\",\n",
    "        np.round(accuracy, 2),\n",
    "        \"F1 Score\",\n",
    "        np.round(f1score, 2),\n",
    "        \"Precision\",\n",
    "        np.round(precision, 2),\n",
    "        \"Recall\",\n",
    "        np.round(recall, 2)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6c26fc27-bbb2-493c-9413-b94209517f23",
  "deepnote_persisted_session": {
   "createdAt": "2022-10-14T15:00:36.060Z"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
